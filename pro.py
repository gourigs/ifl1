# -*- coding: utf-8 -*-
"""pro.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JzHqTJvbPefMhGekqdLMg5o3ZQwWia89
"""


# Import Libraries 

import warnings
warnings.filterwarnings("ignore")

import matplotlib.pyplot as plt
import PIL
import numpy as np
import pandas as pd
import os
from PIL import Image
import matplotlib
from PIL import Image, ImageChops, ImageEnhance
from skimage.io import imread
from skimage import exposure, color
from skimage.transform import resize
from skimage.io import imread, imshow, concatenate_images
from skimage.transform import resize
from skimage.morphology import label
from itertools import chain
from sklearn.model_selection import train_test_split
import tensorflow as tf
import keras
from keras import backend as K
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Model, load_model
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout
from keras.layers.core import Lambda, RepeatVector, Reshape
from numpy import save,load


from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

from tensorflow.keras.optimizers import Adam,SGD
from keras import optimizers

from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from keras.models import load_model

from keras.applications.resnet import ResNet50
from keras.applications.resnet import ResNet101

from sklearn.metrics import roc_curve, auc,roc_auc_score

from tqdm import tqdm
import cv2

from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout
from keras.layers.core import Lambda, RepeatVector, Reshape
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D
from keras.layers import concatenate

path_original = '/content/drive/MyDrive/phase-01/training/pristine/'
path_tampered = '/content/drive/MyDrive/phase-01/training/fake/'
dataset_path = '/content/drive/MyDrive/phase-01/training/'
total_original = os.listdir(path_original)
total_tampered = os.listdir(path_tampered)

#https://stackoverflow.com/questions/47645115/oserror-cannot-identify-image-file-dataset-ds-store
total_tampered.remove('.DS_Store')

print('total number of pristine and tampered images are respectively:',len(total_original),',',len(total_tampered))

#saving the path along with the file names
pristine_images = []
for i in total_original:
    pristine_images.append(dataset_path+i)
fake_images = []
for i in total_tampered:
    fake_images.append(dataset_path+i)

def mask_pristine(path):
    img = Image.open(path).convert("RGB")
    img_shape=(np.array(img)).shape
    return np.ones((img_shape))*255

def plot_ground_truth_mask(image,fake=True):
    if fake:
        PATH=path_tampered+image.replace('.mask','')
    else:
        PATH=path_original+image

    PATH_mask=PATH[:-3]+'mask.png'
    
    img = Image.open(PATH).convert("RGB")
    
    try:
        mask_img=Image.open(PATH_mask).convert("RGB")
    except:
        mask_img=mask_pristine(PATH)
    fig = plt.figure(figsize=(15,10))
    ax1 = fig.add_subplot(221)
    ax2 = fig.add_subplot(222)
    ax1.set_title("Image")
    ax2.set_title("Ground Truth Mask")
    ax1.imshow(img)
    ax2.imshow(mask_img)

'd507e807f025f09ea0cff40b52e9322c.mask.png'.replace('.mask','')[0:-4]

len(total_tampered)

if not os.path.exists(dataset_path+"resized_images/"):
    os.makedirs(dataset_path+"resized_images/fake_masks/")
    os.makedirs(dataset_path+"resized_images/image/fake_images/")
    os.makedirs(dataset_path+"resized_images/image/pristine_images/")
    height = 512
    width = 512
    for fake_image in tqdm(total_tampered):
        
        if('.mask' in fake_image):
            img=Image.open(path_tampered + fake_image).convert("RGB")
            
        
            img = img.resize((height, width), PIL.Image.ANTIALIAS)
            img.save(dataset_path+"resized_images/fake_masks/"+fake_image)
        else:
            
            img=Image.open(path_tampered + fake_image).convert("RGB")
            
            img = img.resize((height, width), PIL.Image.ANTIALIAS)
            img.save(dataset_path+"resized_images/image/fake_images/"+fake_image)
            
    for pristine_image in tqdm(total_original):
        img=Image.open(path_original + pristine_image).convert("RGB")
        
        img = img.resize((height, width), PIL.Image.ANTIALIAS)
        img.save(dataset_path+"resized_images/image/pristine_images/"+pristine_image)
        
        
else:
    print('images resized,path exists')

len(os.listdir(dataset_path+"resized_images/image/pristine_images/"))

resized_fakes = os.listdir(dataset_path+"resized_images/image/fake_images/")

resized_fake_path = dataset_path+"resized_images/image/fake_images/"

len(resized_fakes)



#https://gist.github.com/cirocosta/33c758ad77e6e6531392
#error level analysis of an image
def ELA(img_path):
    """Performs Error Level Analysis over a directory of images"""
    
    TEMP = 'ela_' + 'temp.jpg'
    SCALE = 10
    original = Image.open(img_path)
    try:
        original.save(TEMP, quality=90)
        temporary = Image.open(TEMP)
        diff = ImageChops.difference(original, temporary)
        
    except:
        
        original.convert('RGB').save(TEMP, quality=90)
        temporary = Image.open(TEMP)
        diff = ImageChops.difference(original.convert('RGB'), temporary)
        
       
    d = diff.load()
    
    WIDTH, HEIGHT = diff.size
    for x in range(WIDTH):
        for y in range(HEIGHT):
            d[x, y] = tuple(k * SCALE for k in d[x, y])
        save_path = dataset_path +'ELA_IMAGES/'
        diff.save(save_path+'diff.png')
    return diff

if not os.path.exists(dataset_path+'ELA_IMAGES/'):
    os.makedirs(dataset_path+'ELA_IMAGES/')
    for i in tqdm(resized_fakes):
        ELA(resized_fake_path+i).save(dataset_path+'ELA_IMAGES/'+i)
else:
    print('Images are already converted to ELA')

#Files with the whole path:
[dataset_path+"resized_images/fake_masks/"+i for i in os.listdir(dataset_path+"resized_images/fake_masks/") ][0:10]

ELA_images_with_path = [dataset_path+'ELA_IMAGES/'+i for i in os.listdir(dataset_path+'ELA_IMAGES/') ]
fake_mask_with_path = [dataset_path+"resized_images/fake_masks/"+i for i in os.listdir(dataset_path+"resized_images/fake_masks/") ]

ELA_images_with_path.sort()

fake_mask_with_path.sort()

fake_mask_with_path[0]

def ela_and_mask(index):
   
    fig = plt.figure(figsize=(15,10))
    ax1 = fig.add_subplot(331)
    ax2 = fig.add_subplot(332)
    ax3 = fig.add_subplot(333)
    ax1.set_title("tampered")
    ax2.set_title("ELA")
    ax3.set_title("Ground Truth Mask")
    
    ela_fake = Image.open(ELA_images_with_path[index])
    fake_mask = Image.open(fake_mask_with_path[index])
    tampered_image = Image.open(path_tampered+ELA_images_with_path[index][83:])
    ax1.imshow(tampered_image)
    ax2.imshow(ela_fake)
    ax3.imshow(fake_mask)

ela_and_mask(7)

X_train, X_val, Y_train, Y_val = train_test_split(ELA_images_with_path,fake_mask_with_path , test_size=0.12, random_state=7)

#citation: https://albumentations.readthedocs.io/en/latest/examples.html
#https://github.com/albumentations-team/albumentations/blob/master/notebooks/example.ipynb


# the following code is copied directly from the documentation site : 



## https://github.com/albu/albumentations




from albumentations import *

def strong_aug(p=1):
    return Compose([
        RandomRotate90(),
        
        Transpose(),
        OneOf([
            IAAAdditiveGaussianNoise(),
            GaussNoise(),
        ], p=0.2),
        OneOf([
            MotionBlur(p=.2),
            MedianBlur(blur_limit=3, p=.1),
            Blur(blur_limit=3, p=.1),
        ], p=0.2),
        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=.2),
        OneOf([
            OpticalDistortion(p=0.3),
            GridDistortion(p=.1),
            IAAPiecewiseAffine(p=0.3),
        ], p=0.2),
        OneOf([
            CLAHE(clip_limit=2),
            IAASharpen(),
            IAAEmboss(),
            RandomContrast(),

        ], p=0.3),

    ], p=p)

def aug_with_crop(crop_prob = 1):
    return Compose([

        HorizontalFlip(p=0.5),
        VerticalFlip(p=0.5),
        RandomRotate90(p=0.5),
        Transpose(p=0.5),
        ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),
        RandomBrightnessContrast(p=0.5),

        IAAEmboss(p=0.25),
        Blur(p=0.01, blur_limit = 3),
        OneOf([
            ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),
            GridDistortion(p=0.5),
            OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)                  
        ], p=0.8)
    ], p = 1)

def augment_flips_color(p=.5):
    return Compose([
        CLAHE(),
        RandomRotate90(),
        Transpose(),
        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),
        Blur(blur_limit=3),
        OpticalDistortion(),
        GridDistortion(),

    ], p=p)

def aug_data_1(p=.5):
    return Compose([
        CLAHE(),
        Transpose(),
        Blur(blur_limit=3),
        OpticalDistortion(),
        ElasticTransform(),
        HueSaturationValue()
    ], p=p)

def aug_data_2(p=.5):
    return Compose([
        CLAHE(),
        Blur(blur_limit=3),
        GridDistortion(),
        ElasticTransform(),
        HueSaturationValue()
    ], p=p)

X_val[45]

def horizontalFlip(image,mask):
    aug = HorizontalFlip(p=1)
    augmented = aug(image=image, mask=mask)
    image_hflip = augmented['image']
    mask_hflip = augmented['mask']
    return image_hflip,mask_hflip

def verticalFlip(image,mask):
    aug = VerticalFlip(p=1)
    augmented = aug(image=image, mask=mask)
    image_vflip = augmented['image']
    mask_vflip = augmented['mask']
    return image_vflip,mask_vflip
    
def randomRotate(image,mask):
    aug = RandomRotate90(p=1)
    augmented = aug(image=image, mask=mask)
    image_rot90 = augmented['image']
    mask_rot90 = augmented['mask']
    return image_rot90,mask_rot90
    
def transpose(image,mask):
    aug = Transpose(p=1)
    augmented = aug(image=image, mask=mask)
    image_transpose = augmented['image']
    mask_transpose = augmented['mask']
    return image_transpose,mask_transpose

def elasticDistortion(image,mask):
    aug = ElasticTransform(p=1, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03)
    augmented = aug(image=image, mask=mask)
    image_ed = augmented['image']
    mask_ed = augmented['mask']
    return image_ed,mask_ed

Y_val[45]

def opticalDistortion(image,mask):
    aug = OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)
    augmented = aug(image=image, mask=mask)
    image_od = augmented['image']
    mask_od = augmented['mask']
    return image_od,mask_od

def gridDistortion(image,mask):
    aug = GridDistortion()
    augmented = aug(image=image, mask=mask)
    image_od = augmented['image']
    mask_od = augmented['mask']
    return image_od,mask_od

def strong_Aug(image,mask):
    aug = strong_aug(p=1)
    augmented = aug(image=image, mask=mask)
    image_od = augmented['image']
    mask_od = augmented['mask']
    return image_od,mask_od

def aug_with_Crop(image,mask):
    aug = aug_with_crop()
    augmented = aug(image=image, mask=mask)
    image_od = augmented['image']
    mask_od = augmented['mask']
    return image_od,mask_od

def augment_flips_Color(image,mask):
    aug = augment_flips_color()
    image_od = aug(image=image)['image']
    mask_od = aug(image=mask)['image']

    return image_od,mask_od

def aug_Data_1(image,mask):
    aug = aug_data_1()

    image_od = aug(image=image)['image']
    mask_od = aug(image=mask)['image']

    return image_od,mask_od

def aug_Data_2(image,mask):
    aug = aug_data_2()

    image_od = aug(image=image)['image']
    mask_od = aug(image=mask)['image']

    return image_od,mask_od

def channelShuffle(image,mask):
    aug = ChannelShuffle(p=1)
    augmented = aug(image=image, mask=mask)
    image_od = augmented['image']
    mask_od = augmented['mask']
    return image_od,mask_od

def randomBrightness(image,mask):

    aug = RandomBrightness()

    image_od = aug(image=image)['image']
    mask_od = aug(image=mask)['image']

    return image_od,mask_od

def rotate(image,mask):

    aug = Rotate()
    augmented = aug(image=image, mask=mask)
    image_od = augmented['image']
    mask_od = augmented['mask']
    return image_od,mask_od
  
def rGBShift(image,mask):

    aug = RGBShift()
    augmented = aug(image=image, mask=mask)
    image_od = augmented['image']
    mask_od = augmented['mask']
    return image_od,mask_od

def randomGamma(image,mask):
   
    aug = RandomGamma()
    augmented = aug(image=image, mask=mask)
    image_od = augmented['image']
    mask_od = augmented['mask']
    return image_od,mask_od

def flip(image,mask):
    aug = Flip()
    augmented = aug(image=image, mask=mask)
    image_od = augmented['image']
    mask_od = augmented['mask']
    return image_od,mask_od



def hueSaturationValue(image,mask):
   
    aug = HueSaturationValue()
    augmented = aug(image=image, mask=mask)
    image_od = augmented['image']
    mask_od = augmented['mask']
    return image_od,mask_od

train_data = [i for i in X_train]

mask_data = [i for i in Y_train]

image = cv2.imread(train_data[3]);mask = cv2.imread(mask_data[3]) #converts the image into array

image_aug = horizontalFlip(image,mask)[0]  #augmenting operation
mask_aug = horizontalFlip(image,mask)[1]

len(horizontalFlip(image,mask))

print('Horizontal Flip')
fig = plt.figure(figsize=(15,10))
ax1 = fig.add_subplot(221)
ax2 = fig.add_subplot(222)
ax3 = fig.add_subplot(223)
ax4 = fig.add_subplot(224)
ax1.set_title("ELA")
ax2.set_title("ELA Augmented")
ax3.set_title("Mask")
ax4.set_title("Mask Augmented")
ax1.imshow(image)
ax2.imshow(image_aug)
ax3.imshow(mask)
ax4.imshow(mask_aug)

len(train_data),len(mask_data)

if not os.path.isfile(dataset_path+"phase2/"):
    os.makedirs(dataset_path+"phase2/ela_aug/")
    os.makedirs(dataset_path+"phase2/mask_aug/")

######Performing various types of augmentations on Train data



for i in tqdm(range(len(train_data))):
    
  
    image=cv2.imread(train_data[i]);mask=cv2.imread(mask_data[i])
    ela_name = train_data[i][83:] ; mask_name = train_data[i][83:].replace('.png','.mask.png')
    
    Image.fromarray(image).save(dataset_path+'phase2/ela_aug/'+ 'or_' +  ela_name)  
    Image.fromarray(image).save(dataset_path+'phase2/mask_aug/'+ 'or_'+ mask_name)

    
    
  
    fl=flip(image,mask)
    Image.fromarray(fl[0]).save(dataset_path+'phase2/ela_aug/'+ 'fl_' + ela_name)
    Image.fromarray(fl[1]).save(dataset_path+'phase2/mask_aug/'+ 'fl_' + mask_name)



    hf=horizontalFlip(image,mask)       
    Image.fromarray(hf[0]).save(dataset_path+'phase2/ela_aug/'+ 'hf_' + ela_name)
    Image.fromarray(hf[1]).save(dataset_path+'phase2/mask_aug/'+ 'hf_' + mask_name)


    vf=verticalFlip(image,mask)
    Image.fromarray(vf[0]).save(dataset_path+'phase2/ela_aug/'+ 'vf_' + ela_name)
    Image.fromarray(vf[1]).save(dataset_path+'phase2/mask_aug/'+ 'vf_' + mask_name)


    tp=transpose(image,mask)
    Image.fromarray(tp[0]).save(dataset_path+'phase2/ela_aug/'+ 'tp_' + ela_name)
    Image.fromarray(tp[1]).save(dataset_path+'phase2/mask_aug/'+ 'tp_' + mask_name)


    rr=randomRotate(image,mask)
    Image.fromarray(rr[0]).save(dataset_path+'phase2/ela_aug/'+ 'rr_' + ela_name)
    Image.fromarray(rr[1]).save(dataset_path+'phase2/mask_aug/'+ 'rr_' + mask_name)


    od=opticalDistortion(image,mask)
    Image.fromarray(od[0]).save(dataset_path+'phase2/ela_aug/'+ 'od_' + ela_name)
    Image.fromarray(od[1]).save(dataset_path+'phase2/mask_aug/'+ 'od_' + mask_name)


    ed=elasticDistortion(image,mask)
    Image.fromarray(ed[0]).save(dataset_path+'phase2/ela_aug/'+ 'ed_' + ela_name)
    Image.fromarray(ed[1]).save(dataset_path+'phase2/mask_aug/'+ 'ed_' + mask_name)


    sa=strong_Aug(image,mask)
    Image.fromarray(sa[0]).save(dataset_path+'phase2/ela_aug/'+ 'sa_' + ela_name)
    Image.fromarray(sa[1]).save(dataset_path+'phase2/mask_aug/'+ 'sa_' + mask_name)


    ch=channelShuffle(image,mask)
    Image.fromarray(ch[0]).save(dataset_path+'phase2/ela_aug/'+ 'ch_' + ela_name)
    Image.fromarray(ch[1]).save(dataset_path+'phase2/mask_aug/'+ 'ch_' + mask_name)


    ac=aug_with_Crop(image,mask)
    Image.fromarray(ac[0]).save(dataset_path+'phase2/ela_aug/'+ 'ac_' + ela_name)
    Image.fromarray(ac[1]).save(dataset_path+'phase2/mask_aug/'+ 'ac_' + mask_name)


    af=augment_flips_Color(image,mask)
    Image.fromarray(af[0]).save(dataset_path+'phase2/ela_aug/'+ 'af_' + ela_name)
    Image.fromarray(af[1]).save(dataset_path+'phase2/mask_aug/'+ 'af_' + mask_name)


    gd=gridDistortion(image,mask)
    Image.fromarray(gd[0]).save(dataset_path+'phase2/ela_aug/'+ 'gd_' + ela_name)
    Image.fromarray(gd[1]).save(dataset_path+'phase2/mask_aug/'+ 'gd_' + mask_name)


    ad=aug_Data_1(image,mask)
    Image.fromarray(ad[0]).save(dataset_path+'phase2/ela_aug/'+ 'ad_' + ela_name)
    Image.fromarray(ad[1]).save(dataset_path+'phase2/mask_aug/'+ 'ad_' + mask_name)


#     ad2=aug_Data_2(image,mask)
#     Image.fromarray(ad2[0]).save(dataset_path+'phase2/ela_aug/'+ 'd2_' + ela_name)
#     Image.fromarray(ad2[1]).save(dataset_path+'phase2/mask_aug/'+ 'd2_' + mask_name)


    
#     hs=hueSaturationValue(image,mask)
#     Image.fromarray(hs[0]).save(dataset_path+'phase2/ela_aug/'+ 'hs_' + ela_name)
#     Image.fromarray(hs[1]).save(dataset_path+'phase2/mask_aug/'+ 'hs_' + mask_name)


#     rb=randomBrightness(image,mask)
#     Image.fromarray(rb[0]).save(dataset_path+'phase2/ela_aug/'+ 'rb_' + ela_name)
#     Image.fromarray(rb[1]).save(dataset_path+'phase2/mask_aug/'+ 'rb_' + mask_name)

#     r=rotate(image,mask)
#     Image.fromarray(r[0]).save(dataset_path+'phase2/ela_aug/'+ 'r_' + ela_name)
#     Image.fromarray(r[1]).save(dataset_path+'phase2/mask_aug/'+ 'r_' + mask_name)

#     rgb=rGBShift(image,mask)
#     Image.fromarray(rgb[0]).save(dataset_path+'phase2/ela_aug/'+ 'rg_' + ela_name)
#     Image.fromarray(rgb[1]).save(dataset_path+'phase2/mask_aug/'+ 'rg_' + mask_name)


#     rgm=randomGamma(image,mask)
#     Image.fromarray(rgm[0]).save(dataset_path+'phase2/ela_aug/'+ 'gm_' + ela_name)
#     Image.fromarray(rgm[1]).save(dataset_path+'phase2/mask_aug/'+ 'gm_' + mask_name)

phase2_ela_aug = [dataset_path+'phase2/ela_aug/'+i for i in os.listdir(dataset_path+'phase2/ela_aug/')]

phase2_masks = [dataset_path+'phase2/mask_aug/'+i for i in os.listdir(dataset_path+'phase2/mask_aug/')]

phase2_ela_aug.sort()

phase2_masks.sort()

# final_X_train = phase2_ela_aug + X_train

# final_Y_train = phase2_masks + Y_train

# final_X_train.sort()
# final_Y_train.sort()

## save all the train and validation files into a text file using pickle
import pickle
with open("X_train.txt", "wb") as f:   #Pickling
    pickle.dump(X_train, f) 

## save all the converted text into a text file using pickle
with open("Y_train.txt", "wb") as f:   #Pickling
    pickle.dump(Y_train, f) 
    
with open("X_val.txt", "wb") as f:   #Pickling
    pickle.dump(X_val, f) 

## save all the converted text into a text file using pickle
with open("Y_val.txt", "wb") as f:   #Pickling
    pickle.dump(Y_val, f)

# import pickle
# ## save all the train and validation files into a text file using pickle
# with open("final_X_train.txt", "wb") as f:   #Pickling
#     pickle.dump(final_X_train, f) 

# ## save all the converted text into a text file using pickle
# with open("final_Y_train.txt", "wb") as f:   #Pickling
#     pickle.dump(final_Y_train, f) 
    
    

## save all the train and validation files into a text file using pickle
import pickle
with open("X_val.txt", "wb") as f:   #Pickling
    pickle.dump(X_val, f) 

## save all the converted text into a text file using pickle
with open("Y_val.txt", "wb") as f:   #Pickling
    pickle.dump(Y_val, f) 
    
with open("X_train.txt", "wb") as f:   #Pickling
    pickle.dump(X_train, f) 

## save all the converted text into a text file using pickle
with open("Y_train.txt", "wb") as f:   #Pickling
    pickle.dump(Y_train, f) 
    
    
    

# #run this directly 
import pickle
with open("X_val.txt", "rb") as f:   # Unpickling
    X_val = pickle.load(f)
    
with open("Y_val.txt", "rb") as f:   # Unpickling
    Y_val = pickle.load(f)

# with open("final_X_train.txt", "rb") as f:   # Unpickling
#     final_X_train = pickle.load(f)
# with open("final_Y_train.txt", "rb") as f:   # Unpickling
#     final_Y_train = pickle.load(f)

def metric(y_true, y_pred, smooth=1): # Dice_Coeff or F-Score
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def LoadImages(batch):
    return np.array([resize(imread(file_name), (512, 512, 3)) for file_name in batch])
#https://stackoverflow.com/questions/47200146/keras-load-images-batch-wise-for-large-dataset
def loadImagesBatchwise(X_train,Y_train, batch_size):
    train_image_files=X_train
    train_mask_files=Y_train
    L = len(train_image_files)
    while True:
        batch_start = 0
        batch_end = batch_size
        
        while batch_start < L:
            limit = min(batch_end, L)
            X = LoadImages(train_image_files[batch_start:limit])
            Y = LoadImages(train_mask_files[batch_start:limit])
            yield (X,Y)
            batch_start += batch_size
            batch_end += batch_size



import segmentation_models as sm

sm.set_framework('tf.keras')

sm.framework()

# https://github.com/qubvel/segmentation_models
from segmentation_models import Unet
model = Unet('resnet101', input_shape=(512, 512, 3), classes=3, activation='sigmoid',encoder_weights='imagenet')


model.compile(optimizer=tf.keras.optimizers.Adam(), loss="binary_crossentropy", metrics=[metric])

model.summary()

# final_X_train[13]

X_tr, X_t, Y_tr, Y_t = train_test_split(phase2_ela_aug,phase2_masks , test_size=0.3, random_state=7)

Y_tr[20]

#the training doesn't even start for more points or even worse,the kernel crashes when  datapoints are increased.

X_tr = X_tr[0:528]
X_t = X_t[0:528]
Y_tr = Y_tr[0:528]
Y_t = Y_t[0:528]

from math import ceil

batch_size=4
num_training_samples=len(X_tr)
num_validation_samples=len(X_t)
# steps = ceil(len(X_train)//batch_size)
num_epochs=10
os.makedirs('model_checkpoints')
# define callbacks for learning rate scheduling and best checkpoints saving
filepath = 'model_checkpoints/model_phase_2.hdf5'
checkpoint = keras.callbacks.ModelCheckpoint(filepath,monitor='val_metric',save_best_only=True, mode='max')

early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.22, patience = 1, verbose = 1, min_delta = 0.0001)

# train model
results=model.fit_generator(loadImagesBatchwise(X_tr,Y_tr,batch_size),steps_per_epoch=(num_training_samples // batch_size), epochs=num_epochs,
                            validation_data=loadImagesBatchwise(X_t,Y_t,batch_size),validation_steps=num_validation_samples//batch_size,
                         verbose=1,callbacks=[early_stop,reduce_lr,checkpoint])

model.save('new_model_phase2.hdf5')


fig = plt.figure()
p1 = fig.add_subplot(221)
p2 = fig.add_subplot(222)
p3 = fig.add_subplot(223)
p4 = fig.add_subplot(224)
p2.set_ylim(0,1)
p4.set_ylim(0,1)
p1.grid()
p2.grid()
p3.grid()
p4.grid()
p2.set_yticks(np.arange(0,1,0.1))
p4.set_yticks(np.arange(0,1,0.1))
x = [i for i in range(5)]
y = results.history['loss']
y2 = results.history['metric']
y3 = results.history['val_loss']
y4 = results.history['val_metric']
p1.plot(x,y) #'r', label='train_loss')
p1.legend()
p2.plot(x,y2)# 'b', label='metric')
p2.legend()
p3.plot(x,y3)#, 'r', label='val_loss')
p3.legend()
p4.plot(x,y4)#, 'b', label='val_metric')
p4.legend()
plt.show()

model.load_weights('model_checkpoints/model_phase_2.hdf5')

def load_trained_model() :
  weight_file = f"{os.getcwd()}/model_phase_2.h5"
  assert os.path.isfile(weight_file), "ERROR: fail to locate the pretrained weight file"


test_images=LoadImages(X_t)
predicted=model.predict(test_images)

def plot_predicted_images(index):
    """Plots the predicted masks of tampered images"""
    #ret, bw_img = cv2.threshold((predicted[index]*255),127,255,cv2.THRESH_BINARY)
    plt.imsave('pred_mask.png',predicted[index])
    im_gray = cv2.imread('pred_mask.png', cv2.IMREAD_GRAYSCALE)
    (thresh, im_bw) = cv2.threshold(im_gray, 220, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    #imshow(im_bw)
    fig = plt.figure(figsize=(20,10))
    ax1 = fig.add_subplot(441)
    ax2 = fig.add_subplot(442)
    ax3 = fig.add_subplot(443)
    ax4 = fig.add_subplot(444)
    
    ax1.set_title("actual_image")
    ax2.set_title("actual_mask")
    ax3.set_title("predicted_mask")
    ax4.set_title("binary_predicted_mask")
    actual_img = imread(path_tampered+X_t[index][83:])

    actual_mask = imread(Y_t[index])
    #predicted_mask = imread(predicted[0])

    
    ax1.imshow(actual_img)
    ax2.imshow(actual_mask)
    ax3.imshow(predicted[index])
    ax4.imshow(im_bw)